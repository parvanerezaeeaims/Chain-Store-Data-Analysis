{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, weekofyear, dayofweek, to_date\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n",
    "\n",
    "# Load transactional data\n",
    "data = spark.read.csv(\"/home/ccp/Desktop/task1/Stores_Transactions .csv\", header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "# Convert `transaction_date` to date format\n",
    "data = data.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Add additional date features\n",
    "data = data.withColumn(\"week\", weekofyear(col(\"transaction_date\")))\n",
    "data = data.withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\")))\n",
    "\n",
    "# Aggregate data per day, store, and product\n",
    "daily_sales = data.groupBy(\"store\", \"product_name\", \"transaction_date\", \"week\", \"day_of_week\")\\\n",
    "    .agg({\"quantity\": \"sum\"})\\\n",
    "    .withColumnRenamed(\"sum(quantity)\", \"quantity\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "split_date = \"1403-01-05\"  # Adjust based on your needs\n",
    "train_data = daily_sales.filter(col(\"transaction_date\") <= split_date)\n",
    "test_data = daily_sales.filter(col(\"transaction_date\") > split_date)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pd_train_data = train_data.toPandas()\n",
    "\n",
    "# Encode categorical features\n",
    "store_indexer = StringIndexer(inputCol=\"store\", outputCol=\"store_index\").setHandleInvalid(\"keep\")\n",
    "product_indexer = StringIndexer(inputCol=\"product_name\", outputCol=\"product_index\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# Feature selection\n",
    "feature_cols = [\"store_index\", \"product_index\", \"week\", \"day_of_week\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"quantity\", maxBins=179)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[store_indexer, product_indexer, assembler, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate MAE\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = mae_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Get total sales for week 39\n",
    "weekly_sales = test_predictions.groupBy(\"week\").agg({\"prediction\": \"sum\"})\n",
    "predicted_sales_w39 = weekly_sales.filter(col(\"week\") == 20).select(\"sum(prediction)\").collect()[0][0]\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'setCallSite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1262\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m   1263\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/traceback_utils.py:75\u001b[0m, in \u001b[0;36mSCCallSiteSync.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_site)\n\u001b[1;32m     76\u001b[0m     SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setCallSite'"
     ]
    }
   ],
   "source": [
    "test_predictions.toPandas().head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
